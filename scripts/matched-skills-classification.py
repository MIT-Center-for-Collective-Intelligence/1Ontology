"""
Script Name: classify_ai_applications.py

Purpose:
---------
This Python script automates the classification of AI-related applications or skills
using an ontology service and GPT-5. It helps identify how a given application
(or skill) aligns with an ontology of human/AI activities.

Specifically, for each AI application (or skill entry) in an input CSV file, the script:
  1. Retrieves ontology nodes relevant to the application's main activity using
     the ontology API (https://1ontology.com/api/load-sub-ontology).
  2. Generates a GPT-5 prompt to classify the application's activity into the
     most appropriate ontology node.
  3. Interprets GPT-5’s structured JSON response to determine the closest
     generalization node and rationale.
  4. Tracks GPT token usage, execution time, and estimated cost.
  5. Writes all results to an output CSV file.

Key Classification Tasks:
-------------------------
For each application (or skill):
  - Identify the **main substantive activity** (verb + object).
  - Determine the **relationship** between the system and human performer
    (i.e., does the AI perform or assist?).
  - Select the **most appropriate ontology node** that generalizes the activity.

Workflow:
---------
1. Load input CSV file with columns that include AI application names, taglines,
   and descriptions.
2. For each row (application):
   a. Retrieve relevant ontology nodes via the ontology API.
   b. Construct a GPT-5 prompt with the ontology structure and activity description.
   c. Send the prompt to GPT-5, receive structured JSON output.
   d. Validate and extract the classification details.
   e. Record reasoning, ontology node name, token usage, and estimated cost.
3. Save all classification results in an output CSV file.

Input and Output:
-----------------
Input:
- CSV file containing a "raw_skill" column (list of skill dictionaries)
  Each skill dictionary contains:
    {
      "name": "Skill or application name",
      "description": "Skill description text"
    }

Output:
- CSV file containing:
    - Skill name
    - Skill description
    - Generalization node (ontology)
    - Rationale generated by GPT-5
    - Token usage details
    - Estimated GPT cost (USD)

Environment Requirements:
-------------------------
- Python 3.8+
- Required libraries:
    csv, time, json, requests, ast, openai
- Environment variable:
    export OPENAI_API_KEY="YOUR_API_KEY_HERE"

External Dependencies:
----------------------
- OpenAI API (GPT-5 model)
- Ontology API: https://1ontology.com/api/load-sub-ontology

Notes:
------
- The script handles up to 578 skills in one batch (as defined in the dataset).
- API responses are validated and retried up to 3 times for reliability.
- Cost and token usage are estimated for tracking and transparency.
"""

import csv
import time
import json
import requests
import ast
from openai import OpenAI

client = OpenAI()

# URL endpoint for the ontology API used to load a sub-ontology
API_URL = "https://1ontology.com/api/load-sub-ontology"

# Define input and output CSV paths
csv_file_path = "linkedin_entrylevel_finance_healthcare_MAGA.csv"
output_file_path = "output_gpt5_linkedin_entrylevel_finance_healthcare_MAGA.csv.csv"

# Column in CSV that may contain text prompts (not used directly in this version)
prompt_column = "prompt"


def extract_object(s: str):
    """
    Extracts and returns the first valid JSON object found within a string.

    GPT responses sometimes contain text before or after the JSON block.
    This function scans the string character by character to detect and
    isolate the first well-formed JSON object (balanced braces) and
    converts it into a Python dictionary.

    Args:
        s (str): Input string that may contain a JSON object.

    Returns:
        dict | None: Parsed JSON object as a Python dictionary, or None if not found.
    """
    if not s or "{" not in s:
        return None

    start = s.find("{")
    brace_count = 0

    for i in range(start, len(s)):
        if s[i] == "{":
            brace_count += 1
        elif s[i] == "}":
            brace_count -= 1

        # When all braces balance out, attempt to parse JSON
        if brace_count == 0:
            json_str = s[start : i + 1]
            try:
                return json.loads(json_str)
            except json.JSONDecodeError:
                return None

    return None


def send_request_to_gpt5(prompt: str, reasoning_effort: str = "high"):
    """
    Sends a classification prompt to GPT-5 and captures the structured response.

    The function handles the GPT-5 API request, measures execution time,
    extracts token usage, and estimates API costs.

    Args:
        prompt (str): The full GPT-5 prompt (including ontology and activity details).
        reasoning_effort (str): GPT reasoning level (default: "high").

    Returns:
        dict: Structured information including:
            - 'responseObject': Parsed JSON result from GPT.
            - 'usedTokens': Dictionary with token counts (input, output, reasoning, total).
            - 'cost': Dictionary with estimated costs in USD.
            - 'executionTime': Total API call time in milliseconds.
    """
    try:
        start_time = time.time()

        # Send completion request to GPT-5
        completion = client.chat.completions.create(
            model="gpt-5",
            reasoning_effort=reasoning_effort,
            messages=[{"role": "user", "content": prompt}],
        )

        # Extract token usage data from API response
        usage = getattr(completion, "usage", {})
        prompt_tokens = getattr(usage, "prompt_tokens", 0)
        completion_tokens = getattr(usage, "completion_tokens", 0)
        reasoning_tokens = (
            getattr(
                getattr(usage, "completion_tokens_details", {}), "reasoning_tokens", 0
            )
            if hasattr(usage, "completion_tokens_details")
            else 0
        )
        total_tokens = getattr(usage, "total_tokens", prompt_tokens + completion_tokens)

        # Estimate cost (example rates, may need adjustment)
        input_cost_per_1k = 0.00125
        output_cost_per_1k = 0.01
        input_cost = (prompt_tokens / 1000) * input_cost_per_1k
        output_cost = (
            (completion_tokens + reasoning_tokens) / 1000
        ) * output_cost_per_1k
        total_cost = input_cost + output_cost

        end_time = time.time()
        execution_time_ms = int((end_time - start_time))

        # Extract GPT textual content (may include reasoning or JSON)
        text = (
            completion.choices[0].message.content.strip() if completion.choices else ""
        )

        return {
            "responseObject": extract_object(text),
            "usedTokens": {
                "input": str(prompt_tokens),
                "output": str(completion_tokens),
                "thinking": str(reasoning_tokens),
                "total": str(total_tokens),
            },
            "cost": {
                "inputCost": f"{input_cost:.6f}",
                "outputCost": f"{output_cost:.6f}",
                "totalCost": f"{total_cost:.6f}",
                "currency": "USD",
            },
            "executionTime": execution_time_ms,
        }

    except Exception as e:
        # Return default response if GPT request fails
        print({"error": str(e)})
        return {
            "content": "",
            "usage": None,
            "cost": {
                "inputCost": "0",
                "outputCost": "0",
                "totalCost": "0",
                "currency": "USD",
            },
            "executionTime": 0,
        }


def get_generalization_for_skill(
    skill_name: str, description: str, ontology_object: str
):
    """
    Determines the best generalization node in the ontology for a given skill.

    The function constructs a GPT-5 prompt that includes:
    - The skill name and description.
    - The ontology nodes available for classification.

    GPT-5 is asked to select the closest ontology node that generalizes the
    described activity, and provide reasoning in JSON format.

    Args:
        skill_name (str): Name/title of the skill or application.
        description (str): Short text describing the skill's purpose or context.
        ontology_object (str): JSON string of ontology nodes fetched from the API.

    Returns:
        dict | None: If successful, a dictionary containing:
            - closest_generalization_node
            - closest_generalization_node_rationale
            - tokens
            - cost
        Otherwise, returns None.
    """
    try:
        # Build detailed GPT-5 classification prompt
        prompt = f"""
        ## Role:
        You are an analyst that classifies an activity, specifying which of the nodes in the ontology (provided as a JSON structure in the input) is the best generalization for the given activity. Work only with the supplied nodes and their fields; do not infer or invent nodes or properties. 
        
        ## Ontology Definition:
        Each node in our ontology represents a type of action and has these properties:
        - **title** (String) – a unique, concise title.
        - **description** (String) – a detailed explanation of the node, its purpose, scope, and context.
        - **specializations** *(Array of Objects)* – Groups of more specific types of this node. Each object in the array represents a collection of specializations along a common dimension.

        ## Input:
        - Activity Title: "{skill_name}"
        - Activity description: "{description}"
        - Ontology Nodes: {ontology_object}
        
        ## Output:
        Return a single JSON object only (no prose), exactly with these keys and value types:
        {{
          "closest_generalization_node": "the ontology node title that is the closest generalization of this activity, it should be a string",
          "closest_generalization_node_rationale": "your reasoning for choosing this ontology node",
        }}
        """

        response = None
        retries = 3

        # Retry loop for robustness against invalid GPT responses
        for _ in range(retries):
            result = send_request_to_gpt5(prompt=prompt)
            response = result.get("responseObject")
            usedTokens = result["usedTokens"]
            cost = result.get("cost", {})

            required_keys = [
                "closest_generalization_node",
                "closest_generalization_node_rationale",
            ]
            # Validate presence of all required JSON fields
            if response and all(key in response for key in required_keys):
                return {
                    "closest_generalization_node": response[
                        "closest_generalization_node"
                    ],
                    "closest_generalization_node_rationale": response[
                        "closest_generalization_node_rationale"
                    ],
                    "tokens": f"- input: {usedTokens['input']}\n- thinking: {usedTokens['thinking']}\n- output: {usedTokens['output']}",
                    "cost": cost["totalCost"],
                }

            print("Invalid response detected — retrying...")
            response = None

        print("Failed to get valid response after retries.")
        return None

    except Exception as e:
        print({"error": str(e)})
        return None


# ========================== MAIN SCRIPT EXECUTION ============================

# Open the input CSV and create the output CSV
with open(csv_file_path, newline="", encoding="utf-8") as csvfile, open(
    output_file_path, "w", newline="", encoding="utf-8"
) as outfile:

    reader = csv.DictReader(csvfile)

    # Define output columns for the classification results
    fieldnames = [
        "Skill name",
        "Skill Description",
        "Generalization (the appropriate node of the ontology)",
        "Rationale (generated by gpt-5)",
        "Tokens",
        "Cost",
    ]

    writer = csv.DictWriter(outfile, fieldnames=fieldnames)
    writer.writeheader()  # Write CSV header

    csvfile.seek(0)
    reader = csv.DictReader(csvfile)
    progress = 0

    # Process each skill entry in the input CSV
    for i, row in enumerate(reader, start=1):
        raw_skills = ast.literal_eval(row["raw_skill"])
        for skill in raw_skills:
            progress += 1
            print(
                f"\nProcessing skill {progress} out of 578 of row {i}: {skill['name']}"
            )

            # Prepare the search query for ontology API
            searchQuery = f"{skill['name']} \n\n {skill['description']}"
            payload = {
                "searchQuery": searchQuery,
                "applicationName": "final-hierarchy-with-o*net",
                "nodeType": "activity",
                "searchLimit": 100,
            }
            headers = {
                "Content-Type": "application/json",
                "Authorization": "Bearer YOUR_API_KEY_IF_NEEDED",
            }

            # Request ontology data
            print(f"Loading sub-ontology for '{skill['name']}'...")
            response = requests.post(API_URL, headers=headers, data=json.dumps(payload))

            try:
                data = response.json()
                print("Received response from API.")
            except json.JSONDecodeError:
                print("Response is not valid JSON. Using empty data.")
                data = {}

            ontology_object = data.get("ontology_object", {})
            searchResults = data.get("topResults", [])

            # Classify the skill using GPT-5 and ontology data
            print("Classifying current row...")
            generalization_of_skill = get_generalization_for_skill(
                skill_name=skill["name"],
                description=skill["description"],
                ontology_object=json.dumps(ontology_object, indent=2),
            )
            print(generalization_of_skill)

            # Write classification result to the output CSV
            if generalization_of_skill:
                row_to_write = {
                    "Skill name": skill["name"],
                    "Skill Description": skill["description"],
                    "Generalization (the appropriate node of the ontology)": generalization_of_skill[
                        "closest_generalization_node"
                    ],
                    "Rationale (generated by gpt-5)": generalization_of_skill[
                        "closest_generalization_node_rationale"
                    ],
                    "Tokens": generalization_of_skill["tokens"],
                    "Cost": generalization_of_skill["cost"],
                }
                writer.writerow(row_to_write)
                outfile.flush()
            else:
                print(f"Row '{row['Name']}' could not be classified. Skipping writing.")

    print("\nAll rows processed. Output CSV completed.")
