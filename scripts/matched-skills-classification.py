"""
Script Name: matched-skills-classification.py

Purpose:
---------
This Python script automates the classification of skills
using an ontology service and GPT-5. It helps identify how a given
skill aligns with an ontology node.

Specifically, for each skill in an input CSV file, the script:
  1. Retrieves ontology nodes relevant to the application's main activity using
     the ontology API (https://1ontology.com/api/load-sub-ontology).
  2. Generates a GPT-5 prompt to classify the application's activity into the
     most appropriate ontology node.
  3. Interprets GPT-5’s structured JSON response to determine the closest
     generalization node and rationale.
  4. Tracks GPT token usage, execution time, and estimated cost.
  5. Writes all results to an output CSV file.

Key Classification Tasks:
-------------------------
For each skill:
  - Identify the *closest_generalization_node**.

Workflow:
---------
1. Load input CSV file with columns that include AI application names, taglines,
   and descriptions.
2. For each row (application):
   a. Retrieve relevant ontology nodes via the ontology API.
   b. Construct a GPT-5 prompt with the ontology structure and activity description.
   c. Send the prompt to GPT-5, receive structured JSON output.
   d. Validate and extract the classification details.
   e. Record reasoning, ontology node name, token usage, and estimated cost.
3. Save all classification results in an output CSV file.

Input and Output:
-----------------
Input:
- CSV file containing a "raw_skill" column (list of skill dictionaries)
  Each skill dictionary contains:
    {
      "name": "Skill or application name",
      "description": "Skill description text"
    }

Output:
- CSV file containing:
    - Skill name
    - Skill description
    - Generalization node (ontology)
    - Rationale generated by GPT-5
    - Token usage details
    - Estimated GPT cost (USD)

Environment Requirements:
-------------------------
- Python 3.8+
- Required libraries:
    csv, time, json, requests, ast, openai
- Environment variable:
    export OPENAI_API_KEY="YOUR_API_KEY_HERE"

External Dependencies:
----------------------
- OpenAI API (GPT-5 model)
- Ontology API: https://1ontology.com/api/load-sub-ontology

Notes:
------
- The script handles up to 578 skills in one batch (as defined in the dataset).
- API responses are validated and retried up to 3 times for reliability.
- Cost and token usage are estimated for tracking and transparency.
"""



import csv
import time
import json
import requests
import ast
from openai import OpenAI

client = OpenAI()

# URL endpoint for the ontology API used to load a sub-ontology
API_URL = "https://1ontology.com/api/load-sub-ontology"

# Define input and output CSV paths
csv_file_path = "linkedin_entrylevel_finance_healthcare_MAGA.csv"
output_file_path = "output_gpt5_linkedin_entrylevel_finance_healthcare_MAGA.csv"

# Column in CSV that may contain text prompts (not used directly in this version)
prompt_column = "prompt"


def extract_object(s: str):
    """
    Extracts and returns the first valid JSON object found within a string.

    GPT responses sometimes contain text before or after the JSON block.
    This function scans the string character by character to detect and
    isolate the first well-formed JSON object (balanced braces) and
    converts it into a Python dictionary.

    Args:
        s (str): Input string that may contain a JSON object.

    Returns:
        dict | None: Parsed JSON object as a Python dictionary, or None if not found.
    """
    if not s or "{" not in s:
        return None

    start = s.find("{")
    brace_count = 0

    for i in range(start, len(s)):
        if s[i] == "{":
            brace_count += 1
        elif s[i] == "}":
            brace_count -= 1

        # When all braces balance out, attempt to parse JSON
        if brace_count == 0:
            json_str = s[start : i + 1]
            try:
                return json.loads(json_str)
            except json.JSONDecodeError:
                return None

    return None


def send_request_to_gpt5(prompt: str, reasoning_effort: str = "high"):
    """
    Sends a classification prompt to GPT-5 and captures the structured response.

    The function handles the GPT-5 API request, measures execution time,
    extracts token usage, and estimates API costs.

    Args:
        prompt (str): The full GPT-5 prompt (including ontology and activity details).
        reasoning_effort (str): GPT reasoning level (default: "high").

    Returns:
        dict: Structured information including:
            - 'responseObject': Parsed JSON result from GPT.
            - 'usedTokens': Dictionary with token counts (input, output, reasoning, total).
            - 'cost': Dictionary with estimated costs in USD.
            - 'executionTime': Total API call time in milliseconds.
    """
    try:
        start_time = time.time()

        # Send completion request to GPT-5
        completion = client.chat.completions.create(
            model="gpt-5",
            reasoning_effort=reasoning_effort,
            messages=[{"role": "user", "content": prompt}],
        )

        # Extract token usage data from API response
        usage = getattr(completion, "usage", {})
        prompt_tokens = getattr(usage, "prompt_tokens", 0)
        completion_tokens = getattr(usage, "completion_tokens", 0)
        reasoning_tokens = (
            getattr(
                getattr(usage, "completion_tokens_details", {}), "reasoning_tokens", 0
            )
            if hasattr(usage, "completion_tokens_details")
            else 0
        )
        total_tokens = getattr(usage, "total_tokens", prompt_tokens + completion_tokens)

        # Estimate cost (example rates, may need adjustment)
        input_cost_per_1k = 0.00125
        output_cost_per_1k = 0.01
        input_cost = (prompt_tokens / 1000) * input_cost_per_1k
        output_cost = (
            (completion_tokens + reasoning_tokens) / 1000
        ) * output_cost_per_1k
        total_cost = input_cost + output_cost

        end_time = time.time()
        execution_time_ms = int((end_time - start_time))

        # Extract GPT textual content (may include reasoning or JSON)
        text = (
            completion.choices[0].message.content.strip() if completion.choices else ""
        )

        return {
            "responseObject": extract_object(text),
            "usedTokens": {
                "input": str(prompt_tokens),
                "output": str(completion_tokens),
                "thinking": str(reasoning_tokens),
                "total": str(total_tokens),
            },
            "cost": {
                "inputCost": f"{input_cost:.6f}",
                "outputCost": f"{output_cost:.6f}",
                "totalCost": f"{total_cost:.6f}",
                "currency": "USD",
            },
            "executionTime": execution_time_ms,
        }

    except Exception as e:
        # Return default response if GPT request fails
        print({"error": str(e)})
        return {
            "content": "",
            "usage": None,
            "cost": {
                "inputCost": "0",
                "outputCost": "0",
                "totalCost": "0",
                "currency": "USD",
            },
            "executionTime": 0,
        }


def get_generalization_for_skill(
    skill_name: str, description: str, ontology_object: str
):
    """
    Determines the best generalization node in the ontology for a given skill.

    The function constructs a GPT-5 prompt that includes:
    - The skill name and description.
    - The ontology nodes available for classification.

    GPT-5 is asked to select the closest ontology node that generalizes the
    described activity, and provide reasoning in JSON format.

    Args:
        skill_name (str): Name/title of the skill or application.
        description (str): Short text describing the skill's purpose or context.
        ontology_object (str): JSON string of ontology nodes fetched from the API.

    Returns:
        dict | None: If successful, a dictionary containing:
            - closest_generalization_node
            - closest_generalization_node_rationale
            - tokens
            - cost
        Otherwise, returns None.
    """
    try:
        # Build detailed GPT-5 classification prompt
        prompt = f"""
## Role:
You are an analyst that classifies a given skill according to which of the nodes in the ontology (provided as a JSON structure in the input) is the best classification for that skill. Work only with the supplied nodes and their fields; do not infer or invent nodes or properties. 

## Ontology Definition:
Each node in our ontology represents a type of action and has these properties:
- **title** (String) – a unique, concise title.
- **description** (String) – a detailed explanation of the node, its purpose, scope, and context.
- **specializations** (Array of Objects) – collections of more specific types of this node, organized along common dimensions. Each collection contains:
  - **collectionName** (String) – the dimension along which specializations vary.
  - **nodes** (Array of String) – titles of nodes that are specializations along this dimension.

## Input:
- Skill: "{skill_name}"
- Skill Description: '''{description}'''
- Ontology Nodes: {ontology_object}

## Output:
Return a single JSON object only (no prose), exactly with these keys and value types:
{{
  "most_appropriate_node": {{
    "title": "title of the ontology node that best classifies the skill",
    "description": "description of the ontology node"
  }},
  "paths_to_most_appropriate_node": [an array of strings, each representing a path from the root node "Act" to the title of the mode appropriate node in the ontology. For example, if the most appropriate node was "Express information", this array would have these two strings: "Act > Act on what? > Act on information (“Think”) > Create information > Express information", "Act > Act how? > Create > Create information > Express information"],
  "most_appropriate_node_rationale": "Explain your reasoning for choosing this ontology node"
}}

## Constraints:
- Output must be valid JSON: double quotes around all strings, no trailing commas, no extra keys or text.
- Determine the most appropriate node from the ontology by applying the selection criteria below (coverage first, then specificity, then similarity). Choose the most specific node whose scope fully covers the Skill; if multiple nodes meet this, break ties by higher semantic similarity to the Skill.

### Selection Criteria:
Use these criteria (in order):
1. **Coverage**: the node's description fully covers the essence of the action represented by the Skill.
2. **Specificity**: among nodes that cover the Skill, prefer the narrowest scope that still fully covers it.
3. **Similarity**: prefer nodes whose title + description align with the Skill.
4. **Tie-breakers**: if still tied, prefer the node most proximal to the Skill; if still tied, choose the one with clearer alignment to the Skill.

## Procedure:
Internally follow this process:
1. Internally, analyze the Skill and compare it to every node in the ontology. For each node, consider all its information.
2. Select the candidate that passes coverage with the highest overall alignment, breaking ties using the rubric above.
3. Produce the output JSON exactly as specified.
"""

        response = None
        retries = 3

        # Retry loop for robustness against invalid GPT responses
        for _ in range(retries):
            result = send_request_to_gpt5(prompt=prompt)
            response = result.get("responseObject")
            usedTokens = result["usedTokens"]
            cost = result.get("cost", {})

            required_keys = [
                "most_appropriate_node",
                "paths_to_most_appropriate_node",
                "most_appropriate_node_rationale",
            ]
            # Validate presence of all required JSON fields
            if response and all(key in response for key in required_keys):
                return {
                    "closest_generalization_node": response["most_appropriate_node"][
                        "title"
                    ],
                    "closest_generalization_node_rationale": response[
                        "most_appropriate_node_rationale"
                    ],
                    "paths": "\n".join(response["paths_to_most_appropriate_node"]),
                    "tokens": f"- input: {usedTokens['input']}\n- thinking: {usedTokens['thinking']}\n- output: {usedTokens['output']}",
                    "cost": cost["totalCost"],
                }

            print("Invalid response detected — retrying...")
            response = None

        print("Failed to get valid response after retries.")
        return None

    except Exception as e:
        print({"error": str(e)})
        return None


# ========================== MAIN SCRIPT EXECUTION ============================

# Open the input CSV and create the output CSV
with open(csv_file_path, newline="", encoding="utf-8") as csvfile, open(
    output_file_path, "w", newline="", encoding="utf-8"
) as outfile:

    reader = csv.DictReader(csvfile)

    # Define output columns for the classification results
    fieldnames = [
        "Skill name",
        "Skill Description",
        "Generalization (the appropriate node of the ontology)",
        "Rationale (generated by gpt-5)",
        "Paths",
        "Tokens",
        "Cost",
    ]

    writer = csv.DictWriter(outfile, fieldnames=fieldnames)
    writer.writeheader()  # Write CSV header

    csvfile.seek(0)
    reader = csv.DictReader(csvfile)
    progress = 0

    # Process each skill entry in the input CSV
    for i, row in enumerate(reader, start=1):
        raw_skills = ast.literal_eval(row["raw_skill"])
        for skill in raw_skills:
            progress += 1
            print(
                f"\nProcessing skill {progress} out of 578 of row {i}: {skill['name']}"
            )

            # Prepare the search query for ontology API
            searchQuery = f"{skill['name']} \n\n {skill['description']}"
            payload = {
                "searchQuery": searchQuery,
                "applicationName": "final-hierarchy-with-o*net",
                "nodeType": "activity",
                "searchLimit": 100,
            }
            headers = {
                "Content-Type": "application/json",
                "Authorization": "Bearer YOUR_API_KEY_IF_NEEDED",
            }

            # Request ontology data
            print(f"Loading sub-ontology for '{skill['name']}'...")
            response = requests.post(API_URL, headers=headers, data=json.dumps(payload))

            try:
                data = response.json()
                print("Received response from API.")
            except json.JSONDecodeError:
                print("Response is not valid JSON. Using empty data.")
                data = {}

            ontology_object = data.get("ontology_object", {})
            searchResults = data.get("topResults", [])

            # Classify the skill using GPT-5 and ontology data
            print("Classifying current row...")
            generalization_of_skill = get_generalization_for_skill(
                skill_name=skill["name"],
                description=skill["description"],
                ontology_object=json.dumps(ontology_object, indent=2),
            )
            print(generalization_of_skill)

            # Write classification result to the output CSV
            if generalization_of_skill:
                row_to_write = {
                    "Skill name": skill["name"],
                    "Skill Description": skill["description"],
                    "Generalization (the appropriate node of the ontology)": generalization_of_skill[
                        "closest_generalization_node"
                    ],
                    "Rationale (generated by gpt-5)": generalization_of_skill[
                        "closest_generalization_node_rationale"
                    ],
                    "Paths": generalization_of_skill["paths"],
                    "Tokens": generalization_of_skill["tokens"],
                    "Cost": generalization_of_skill["cost"],
                }
                writer.writerow(row_to_write)
                outfile.flush()
            else:
                print(f"Row '{row['Name']}' could not be classified. Skipping writing.")

    print("\nAll rows processed. Output CSV completed.")
